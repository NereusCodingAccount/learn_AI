# ç¥ç¶“ç¶²è·¯æ¶æ§‹é€Ÿè¦½

# ä¸»è¦å…ƒä»¶ï¼ˆç°¡æ½”ï¼‰

## å±¤ï¼ˆLayerï¼‰ï¼šDense/Linearã€Convã€RNNã€Self-Attentionã€‚

### å±¤ï¼ˆLayerï¼‰æ˜¯ç¥ç¶“ç¶²è·¯ä¸­çš„åŸºæœ¬é‹ç®—å–®å…ƒï¼Œç”¨ä¾†æŠŠè¼¸å…¥è½‰æ›æˆè¼¸å‡ºï¼Œä¸¦é€å±¤æŠ½å–æˆ–çµ„åˆç‰¹å¾µã€‚ç°¡è¦èªªæ˜ï¼š

- å¯ä»¥æƒ³åƒç¥ç¶“ç¶²è·¯å°±åƒæ˜¯ä¸€å€‹ã€Œå¤šå±¤çš„æ•¸å­¸å‡½æ•¸ã€ï¼Œæ¯ä¸€å±¤éƒ½æœƒ**æ¥æ”¶è¼¸å…¥** â†’ **åšé‹ç®—** â†’ **å‚³å‡ºçµæœ**ã€‚

### åŸºæœ¬æ•¸å­¸å½¢å¼

è¨±å¤šå±¤å¯è¡¨ç¤ºç‚º y = f(Wx + b)ï¼ˆå¯è¨“ç·´åƒæ•¸ W, bï¼›f ç‚ºå•Ÿå‹•å‡½æ•¸ï¼‰ã€‚
æœ‰äº›å±¤æ˜¯ç„¡åƒæ•¸çš„ï¼ˆä¾‹å¦‚ MaxPoolã€Flattenã€Activation è‹¥å–®ç¨ç®—ä½œå±¤ï¼‰ã€‚
å¸¸è¦‹å±¤é¡å‹ï¼ˆç”¨é€”èˆ‡ç‰¹æ€§ï¼‰

- ### åŸºç¤çµæ§‹å±¤:

- **Dense / Fullyâ€‘connected(å…¨é€£æ¥å±¤)**ï¼šæœ€åŸºæœ¬çš„å±¤ï¼Œæ¯å€‹è¼¸å…¥é€£åˆ°æ¯å€‹è¼¸å‡ºï¼›é©åˆ**è¡¨æ ¼è³‡æ–™ã€åˆ†é¡/å›æ­¸**ã€‚

```python
from tensorflow.keras.layers import Dense

layer = Dense(units=64, activation='relu')
```
---

- **Convolutionï¼ˆConvï¼‰(å·ç©å±¤)**ï¼šç”¨å·ç©æ ¸åœ¨å±€éƒ¨çª—å£è¨ˆç®—ï¼Œä¿ç•™ç©ºé–“çµæ§‹ï¼Œ**å¸¸ç”¨æ–¼å½±åƒ**ã€‚

```python
from tensorflow.keras.layers import Conv2D

layer = Conv2D(filters=32, kernel_size=(3,3), activation='relu')
```
---

- **Poolingï¼ˆMaxPool / AvgPoolï¼‰(æ± åŒ–å±¤)**ï¼šç©ºé–“ä¸‹æ¡æ¨£ï¼Œæ¸›å°‘å°ºå¯¸èˆ‡å¹³ç§»ä¸è®Šæ€§ã€‚ex: **MaxPoolingï¼ˆå–æœ€å¤§å€¼ï¼‰ã€AveragePoolingï¼ˆå–å¹³å‡å€¼ï¼‰**

```python
from tensorflow.keras.layers import MaxPooling2D

layer = MaxPooling2D(pool_size=(2,2))
```
---

- **Dropout (éš¨æ©Ÿä¸Ÿæ£„å±¤)**ï¼šè¨“ç·´æ™‚éš¨æ©Ÿä¸Ÿæ£„ç¥ç¶“å…ƒï¼Œåšæ­£å‰‡åŒ–ï¼ˆç„¡åƒæ•¸ï¼‰ï¼Œé˜²æ­¢æ¨¡å‹éåº¦å­¸ç¿’ã€‚
```python
from tensorflow.keras.layers import Dropout

layer = Dropout(0.5)
```
---

- ### ä¸­é«˜éšå±¤:

- **Recurrentï¼ˆRNN / LSTM / GRUï¼‰(å¾ªç’°å±¤)**ï¼š
 ç”¨ä¾†è™•ç†ã€Œåºåˆ—è³‡æ–™ã€çš„å±¤ï¼Œä¾‹å¦‚**æ–‡å­—ã€æ™‚é–“åºåˆ—ã€èªéŸ³**ã€‚
 
- æ™®é€šçš„Denseåªçœ‹**ç•¶å‰è¼¸å…¥** = æ²’æœ‰è¨˜æ†¶ï¼Œä½†èªå¥æœ‰é †åºçš„ï¼Œé€™å°±é ã€Œhidden stateã€ä¾†å¯¦ç¾

ğŸ”¹ RNN / LSTM / GRU å·®ç•°

RNN	æœ€åŸºæœ¬ï¼Œæœƒè¨˜éŒ„å‰ä¸€å€‹ hidden stateï¼Œç¼ºé»æ˜¯:**å®¹æ˜“ã€Œæ¢¯åº¦æ¶ˆå¤±ã€**
LSTMï¼ŒåŠ å…¥ã€Œè¨˜æ†¶å–®å…ƒã€ï¼ˆcell stateï¼‰ï¼Œèƒ½é•·æœŸè¨˜æ†¶ï¼Œç¼ºé»æ˜¯:**çµæ§‹è¼ƒè¤‡é›œ**
GRUï¼ŒLSTM çš„ç°¡åŒ–ç‰ˆæœ¬ï¼Œæ•ˆèƒ½ç›¸è¿‘ã€é€Ÿåº¦æ›´å¿«ï¼Œç¼ºé»æ˜¯:**å°‘ä¸€å€‹é–€(no cell state)**

```python
from tensorflow.keras.layers import SimpleRNN, LSTM, GRU

# RNN
rnn = SimpleRNN(64, activation='tanh', return_sequences=True)# return_sequences=True â†’ è¡¨ç¤ºè¼¸å‡ºæ¯å€‹æ™‚é–“æ­¥çš„çµæœ

# LSTM
lstm = LSTM(128, return_sequences=False)# False â†’ åªè¼¸å‡ºæœ€å¾Œä¸€å€‹ï¼ˆå¸¸ç”¨æ–¼åˆ†é¡ï¼‰

# GRU
gru = GRU(128, return_sequences=True)
```


- **Attention / Selfâ€‘Attention**ï¼šä»¥æ³¨æ„åŠ›æ©Ÿåˆ¶å»ºæ¨¡åºåˆ—ä¸­å…ƒç´ é–“çš„é—œè¯ï¼ˆTransformerï¼‰ã€‚
ç”¨æ–¼å»ºæ¨¡åºåˆ—ä¸­ã€Œå…ƒç´ å½¼æ­¤é–“çš„é—œè¯æ€§ã€ã€‚
æ˜¯ **Transformerï¼ˆGPTã€BERT ç­‰ï¼‰**çš„é—œéµæŠ€è¡“ã€‚

#### RNN é›–ç„¶èƒ½è¨˜ä½å‰æ–‡ï¼Œä½†è¨˜æ†¶æœƒã€Œè¡°é€€ã€ï¼›Attention å¯ä»¥ç›´æ¥ã€Œæ¯”è¼ƒåºåˆ—ä¸­æ‰€æœ‰ä½ç½®ã€
- #### æ ¸å¿ƒæ¦‚å¿µ:
  æ¯å€‹è¼¸å‡ºç”¢ç”Ÿä¸‰å€‹å‘é‡: **QKV**
    Qï¼ˆQueryï¼‰ï¼šæˆ‘è¦é—œæ³¨ä»€éº¼
    Kï¼ˆKeyï¼‰ï¼šæˆ‘èƒ½æä¾›ä»€éº¼è³‡è¨Š
    Vï¼ˆValueï¼‰ï¼šå¯¦éš›è³‡è¨Šå…§å®¹
    ![alt text](image.png)

ç•¶ Q, K, V éƒ½ä¾†è‡ªåŒä¸€çµ„è¼¸å…¥æ™‚ â†’ **Self-Attention**
é€™è®“æ¨¡å‹èƒ½è‡ªå·±ã€Œçœ‹æ•´å¥è©±ã€Ex:â€œThe **animal** didnâ€™t cross the street because **it** was too tired.â€
æ¨¡å‹æœƒçŸ¥é“ â€œitâ€ æŒ‡çš„æ˜¯ â€œanimalâ€ï¼Œå› ç‚ºæ³¨æ„åŠ›è®“æ¯å€‹è©éƒ½èƒ½ã€Œé—œæ³¨ã€å¥ä¸­å…¶ä»–è©ã€‚

#### å¤šé ­æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰
ç”¨å¤šçµ„(Q,K,V)é€²è¡Œ
```python
from tensorflow.keras.layers import MultiHeadAttention

attn = MultiHeadAttention(num_heads=8, key_dim=64)
output = attn(query=x, value=x, key=x)  # self-attention
```
---


- **Normalizationï¼ˆBatchNorm / LayerNormï¼‰**ï¼š **âœ¨æ¨™æº–åŒ–æ¿€æ´»å€¼**

- 1.**Batch Normalization**
åœ¨ å°æ‰¹æ¬¡è³‡æ–™ï¼ˆbatchï¼‰ ç¶­åº¦ä¸Šæ¨™æº–åŒ–ï¼Œé€šå¸¸ç”¨æ–¼**CNNã€MLP**
```python
from tensorflow.keras.layers import BatchNormalization

bn = BatchNormalization()
```


- 2.**Layer Normalization**
åœ¨ ç‰¹å¾µç¶­åº¦ ä¸Šæ¨™æº–åŒ–ï¼ˆä¸ä¾è³´ batch å¤§å°ï¼‰ï¼Œæ›´é©åˆ **Transformerã€RNN**ã€‚
```python
from tensorflow.keras.layers import LayerNormalization

ln = LayerNormalization()
```
- BatchNormï¼Œå°æ•´å€‹ batch çš„è¼¸å…¥å¹³å‡ï¼Œç”¨åœ¨:**CNNã€MLP**
- LayerNormï¼Œå°æ¯å€‹æ¨£æœ¬çš„æ‰€æœ‰ç‰¹å¾µå¹³å‡ï¼Œç”¨åœ¨:**Transformerã€RNN**

---





### è¼¸å…¥/è¼¸å‡ºå½¢ç‹€ï¼ˆshapeï¼‰è§€å¿µ

- **Dense**ï¼šè¼¸å…¥ (batch, in_dim) â†’ è¼¸å‡º (batch, out_dim)ã€‚

- **Conv2D**ï¼šè¼¸å…¥ (batch, C, H, W) â†’ è¼¸å‡º (batch, C_out, H', W')ï¼ˆä¾ kernelã€strideã€padding æ±ºå®š H', W'ï¼‰ã€‚

- **RNN**ï¼šè¼¸å…¥ (seq_len, batch, feature) æˆ– (batch, seq_len, feature) â†’ è¼¸å‡ºå«æ™‚é–“ç¶­åº¦ã€‚
å±¤çš„è§’è‰²èˆ‡è¨­è¨ˆè¦é»

æ¯å±¤å­¸åˆ°ä¸åŒå±¤æ¬¡çš„è¡¨ç¤ºï¼ˆæ·ºå±¤å­¸å±€éƒ¨/ä½éšç‰¹å¾µï¼Œæ·±å±¤å­¸æŠ½è±¡/é«˜éšç‰¹å¾µï¼‰ã€‚
å±¤çš„é †åºèˆ‡ç¨®é¡æ±ºå®šç¶²è·¯èƒ½åŠ›èˆ‡è¨ˆç®—é‡ï¼ˆä¾‹å¦‚ Convâ†’Poolâ†’Convâ†’Flattenâ†’Denseï¼‰ã€‚
å°å¿ƒåƒæ•¸æ•¸é‡ï¼ˆéå¤šæœƒéæ“¬åˆã€éå°‘è¡¨é”åŠ›ä¸è¶³ï¼‰ï¼›æ­é…æ­£å‰‡åŒ–èˆ‡é©ç•¶æ·±åº¦ã€‚  

## å•Ÿå‹•å‡½æ•¸ï¼šReLUã€Sigmoidã€Tanhã€GELUã€‚  

## æå¤±å‡½æ•¸ï¼šåˆ†é¡â†’Cross-Entropyï¼Œå›æ­¸â†’MSEã€‚  
## å„ªåŒ–å™¨ï¼šSGDã€Adamï¼ˆå¸¸ç”¨ï¼‰ã€‚  
## æ­£å‰‡åŒ–ï¼šDropoutã€BatchNorm/LayerNormã€L2ï¼ˆæ¬Šé‡è¡°æ¸›ï¼‰ã€‚  
## è©•ä¼°æŒ‡æ¨™ï¼šAccuracyã€F1ã€ROC-AUCã€MAE/MSEã€‚

## å¸¸è¦‹æ¶æ§‹ï¼ˆç”¨é€”èˆ‡é‡é»ï¼‰
- MLPï¼ˆå¤šå±¤æ„ŸçŸ¥å™¨ï¼‰ï¼šè¡¨æ ¼è³‡æ–™ã€ç°¡å–®åˆ†é¡/å›æ­¸ã€‚  
- CNNï¼ˆå·ç©ç¶²è·¯ï¼‰ï¼šå½±åƒã€å±€éƒ¨ç‰¹å¾µæ“·å–ã€æ± åŒ–èˆ‡å·ç©æ ¸è¨­è¨ˆå¾ˆé‡è¦ã€‚  
- RNN / LSTM / GRUï¼šåºåˆ—è³‡æ–™ï¼ˆæ–‡å­—ã€æ™‚é–“åºåˆ—ï¼‰ã€‚  
- Transformerï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰ï¼šNLP èˆ‡åºåˆ—å»ºæ¨¡ï¼Œå…·å¹³è¡ŒåŒ–èˆ‡é•·è·é›¢ä¾è³´è™•ç†èƒ½åŠ›ã€‚  
- Autoencoderï¼šé™ç¶­ã€å»å™ªã€è¡¨ç¤ºå­¸ç¿’ã€‚  
- GANï¼šç”Ÿæˆæ¨¡å‹ï¼Œè¨“ç·´éœ€æ³¨æ„ä¸ç©©å®šæ€§èˆ‡æ¨¡å¼å´©æ½°ã€‚  
- GNNï¼šè™•ç†åœ–çµæ§‹è³‡æ–™ï¼ˆæ¶ˆæ¯å‚³éï¼‰ã€‚

## è¨­è¨ˆèˆ‡èª¿åƒè¦é»ï¼ˆå¯¦å‹™é‡é»ï¼‰
- å…ˆå¾å°æ¨¡å‹èˆ‡å°‘é‡å±¤é–‹å§‹ï¼Œé€æ­¥æ“´å¤§ã€‚  
- å­¸ç¿’ç‡æœ€æ•æ„Ÿï¼šä½¿ç”¨ lr èª¿åº¦ã€warmupã€‚  
- ä½¿ç”¨ BatchNorm/LayerNorm å¹«åŠ©ç©©å®šèˆ‡åŠ é€Ÿæ”¶æ–‚ã€‚  
- é¿å…éæ“¬åˆï¼šè³‡æ–™æ“´å¢ã€Dropoutã€æ—©åœï¼ˆearly stoppingï¼‰ã€‚  
- batch size èˆ‡ lr ç›¸é—œï¼Œå¯ç”¨ç·šæ€§ç¸®æ”¾åŸå‰‡èª¿æ•´ã€‚  
- åˆ†è¨“ç·´/é©—è­‰/æ¸¬è©¦é›†ï¼Œç¢ºä¿è©•ä¼°å¯é æ€§ã€‚

## ç°¡çŸ­ç¯„ä¾‹ï¼ˆPyTorchï¼‰

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleMLP(nn.Module):
    def __init__(self, input_dim, hidden=128, num_classes=10):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden)
        self.fc2 = nn.Linear(hidden, num_classes)
    def forward(self, x):
        x = F.relu(self.fc1(x))
        return self.fc2(x)

class SimpleCNN(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.MaxPool2d(2)
        self.fc = nn.Linear(64*8*8, num_classes)  # å‡è¨­è¼¸å…¥ 32x32
    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(x.size(0), -1)
        return self.fc(x)
```

## Transformer Encoderï¼ˆç°¡éª¨æ¶ï¼‰
```python
import torch.nn as nn

class SimpleTransformerEncoder(nn.Module):
    def __init__(self, d_model=512, nhead=8, num_layers=6):
        super().__init__()
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
    def forward(self, src, src_mask=None):
        # src shape: (seq_len, batch, d_model)
        return self.encoder(src, mask=src_mask)
```

---

